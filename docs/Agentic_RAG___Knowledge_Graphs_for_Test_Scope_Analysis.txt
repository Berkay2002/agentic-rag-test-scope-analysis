1 Introduction
Motivation
In the domain of large-scale enterprise software development, the velocity of feature delivery is a critical competitive advantage. Modern Continuous Integration and Continuous Deployment (CI/CD) pipelines have enabled rapid iteration cycles, but they simultaneously impose significant demands on quality assurance processes [1]. A critical bottleneck in this workflow is test scope analysis. In this thesis, we define test scope analysis as the activity of identifying and recommending relevant legacy test cases for verification when a new feature, change request, or defect fix is introduced [2]. This differs from traditional Regression Test Selection (RTS), which typically focuses on code-coverage-based filtering of a test suite for automated execution [3, 4]. Instead, this work addresses the Test Case Recommendation problem: supporting engineers in the semantic discovery of tests that may not be directly linked via static traces but are contextually relevant to the change.
For large organizations, maintaining high test coverage while avoiding redundant execution is a complex optimization problem. When a new artifact, such as a user story or bug ticket, is introduced, test engineers must navigate vast repositories of test cases to identify existing coverage and potential gaps [5]. Historically, this retrieval process has relied on keyword-based search or the manual inspection of static trace links. However, these traditional methods are increasingly brittle; they often fail to capture the semantic nuance of natural language requirements or the complex, graph-like dependencies between system components [6, 7]. As software systems grow in complexity, the "semantic gap" between unstructured change descriptions (e.g., "fix race condition in handover") and structured test cases (e.g., "TC_HO_001") widens, leading to inefficient scoping, missed defects, and increased maintenance costs [8].
Recent advancements in Large Language Models (LLMs), which are AI models capable of understanding and generating human-like text, and Retrieval-Augmented Generation (RAG), a technique that enhances LLMs by fetching relevant information from external data sources [9], offer promising avenues for bridging this semantic gap [10, 11]. Simultaneously, Knowledge Graphs (KGs), structured representations of interconnected entities and their relationships, provide a structured means to model the intricate relationships between requirements, code, and tests [12, 13].
The primary problem addressed in this thesis is the inefficiency and inaccuracy of current test scope analysis methods in large-scale software engineering contexts. Traditional keyword search approaches lack the semantic understanding required to match high-level requirements with low-level test steps, frequently resulting in low recall (missing relevant tests) or low precision (retrieving irrelevant tests).
While standard RAG approaches improve semantic matching, they often suffer from precision errors (retrieving plausible but incorrect context) and "hallucinations" (generative errors where the model invents non-existent facts or test names) [14]. Furthermore, flat RAG retrieval fails to account for the structural context inherent in software artifacts (e.g., inheritance, trace links, execution history). Conversely, purely graph-based methods struggle to effectively process the unstructured free text found in change logs and user stories [15]. Consequently, there is a lack of integrated solutions that leverage the combined strengths of vector-based semantic retrieval and graph-based structural reasoning often referred to as GraphRAG [16] to provide accurate and explainable test scope recommendations.
However, a static GraphRAG pipeline—where a query is simply converted to a vector, matched against a graph, and summarized—remains insufficient for complex software engineering tasks. Such pipelines fail when the initial user intent is ambiguous or requires multi-step investigation (e.g., "Check if this change affects the billing module, and if so, find tests for the legacy payment gateway"). A static retrieval process cannot decompose this intent, perform intermediate lookups to refine the search criteria, or reason about the absence of information. To address this, an agentic architecture is required [17]. By combining these technologies into a system where autonomous AI agents orchestrate retrieval and reasoning loops, there is potential to transform test scope analysis from a manual, error-prone task into an automated, explainable, and highly accurate process [18].
Aim
The aim of this thesis is to design, implement, and evaluate an agent-orchestrated RAG and Knowledge Graph architecture for test scope analysis. The study aims to determine the effectiveness of this hybrid approach in automating the retrieval of relevant legacy test cases, identifying coverage gaps, and providing explainable rationales for its recommendations, thereby increasing practical utility for software practitioners.
Research questions
To fulfill the aim, the following research questions have been formulated:
* RQ1: What ontology design is required to effectively map unstructured requirements to structured test artifacts within a Knowledge Graph to enable semantic retrieval?
* RQ2: To what extent does an agent-orchestrated GraphRAG approach improve retrieval accuracy (measured by precision@k and recall@k) compared to standard keyword-based and vector-only RAG methods?
* RQ3: How do software practitioners perceive the explainability and practical utility of the automated test scope recommendations provided by the system?
Delimitations
This study is delimited to the context of Ericsson's internal software development environment.
* Data Scope: The study will utilize specific datasets provided by Ericsson, including structured test management data (requirements, trace links) and unstructured test case descriptions.
* Evaluation Scope: Performance evaluation is limited to offline metrics (precision@k, recall@k, MAP) and a qualitative human-in-the-loop utility study with a selected group of Ericsson engineers.
* Functional Scope: The system focuses exclusively on Test Scope Analysis (identifying relevant existing tests) and does not include the automatic generation of new test cases or the execution of tests.
Disposition
The remainder of this report is structured as follows:
* Theoretical Framework: Presents the relevant theory concerning LLMs, RAG, Knowledge Graphs, and software testing methodologies.
* Method: Describes the design science research methodology, the system architecture, and the evaluation metrics used.
* Implementation: Details the data ingestion process, Knowledge Graph schema design, and the implementation of the hybrid retrieval agents.
* Results: Presents the quantitative performance metrics from the comparative analysis and qualitative feedback from the practitioner user study.
* Discussion: Analyzes the results in relation to the research questions and related work, discussing the implications for the field.
* Conclusion: Summarizes the key findings and suggests directions for future research.
2 Theoretical Framework
This chapter establishes the theoretical foundation for the agent-orchestrated Retrieval-Augmented Generation (RAG) and Knowledge Graph architecture developed in this thesis. It begins by grounding the problem in the context of modern software development practices and quality assurance, followed by an in-depth exploration of Large Language Models (LLMs), RAG, Knowledge Graphs, and AI Agents, including their constituent components and operational mechanisms.
2.1 Software Development Lifecycle & Quality Assurance
Modern software development is characterized by rapid iteration and continuous delivery, largely facilitated by Continuous Integration and Continuous Deployment (CI/CD) pipelines. These methodologies accelerate feature delivery but place significant demands on quality assurance, particularly in ensuring the reliability of complex systems through effective testing.
Continuous Integration and Continuous Deployment (CI/CD)
CI/CD represents a set of practices that enable rapid and reliable software releases [19]. Continuous Integration involves frequently merging code changes into a central repository, followed by automated builds and tests. Continuous Deployment extends this by automatically deploying verified changes to production. While highly efficient, CI/CD necessitates robust testing strategies to prevent the introduction of defects at an accelerated pace.
Regression Testing and Test Scope Analysis
Central to quality assurance in CI/CD environments is regression testing, which ensures that new code changes do not adversely affect existing functionalities [3]. However, re-running all tests for every small change is resource-intensive and impractical in large-scale systems [4]. This gives rise to the critical challenge of test scope analysis: the activity of intelligently determining the minimal set of legacy test cases relevant for verification when a new feature, change request, or defect fix is introduced. Effective test scope analysis balances the need for high test coverage with the efficiency of execution, preventing redundant tests while identifying potential gaps.
Traceability in Software Engineering
Software traceability refers to the ability to link related artifacts throughout the software development lifecycle, such as requirements to design, design to code, and code to test cases. Robust traceability is fundamental for effective test scope analysis, as it provides the explicit connections necessary to understand the impact of changes and identify corresponding verification activities. However, maintaining accurate and comprehensive traceability in large, evolving systems is a significant challenge.
Observability, Evaluation, and Deployment
Beyond the core development and testing activities, the successful operation of complex software systems, especially those incorporating AI, relies on robust practices for observability, evaluation, and deployment.
* Observability: Refers to the ability to infer the internal states of a system by examining its external outputs [20]. In AI-driven systems, this is crucial for understanding how models and agents make decisions, identifying potential biases, and diagnosing issues in real-time.
* Evaluation: Encompasses the methods and metrics used to assess a system's performance, accuracy, and overall effectiveness. For test scope analysis systems, this includes quantitative metrics such as precision and recall, as well as qualitative assessments of practical utility and explainability.
* Deployment: Is the process of making the developed system available for use, ranging from local integration to large-scale production rollouts. For agentic systems, deployment strategies must consider integration with existing workflows, scalability, and maintenance.
2.2 Large Language Models
Large Language Models (LLMs) form the cognitive backbone of modern AI-driven applications, including agentic systems. These models are advanced neural networks trained on vast datasets of text, enabling them to understand, generate, and process human language with remarkable fluency.
Foundational Principles
At their core, LLMs are built upon the transformer architecture, which allows them to process sequences of data in parallel, capturing long-range dependencies more effectively than previous architectures. However, the field is rapidly evolving beyond standard dense transformers. To improve efficiency and scalability, modern architectures often incorporate techniques like Mixture-of-Experts (MoE), where only a subset of parameters ("experts") are activated for a given input. This approach is explicitly utilized in open-weights models like DeepSeek-R1 [21]. Other frontier models, such as Gemini 3 Pro [22], are explicitly built upon a sparse Mixture-of-Experts (MoE) transformer-based architecture, offering native multimodal support for text, vision, and audio inputs. Similarly, Claude 4.5 Sonnet [23] exhibits advanced "hybrid" reasoning and multimodal capabilities, implying significant architectural innovations, though its specific internal structure remains proprietary.
Attention Mechanism
The Attention Mechanism [24] is a pivotal component of the transformer architecture, enabling LLMs to weigh the importance of different parts of the input sequence when processing each token. Self-Attention, in particular, allows the model to capture dependencies between tokens regardless of their distance in the input, forming a rich contextual representation. This mechanism computes three vectors for each token: a Query (Q), a Key (K), and a Value (V). The attention score for a given Query token is calculated by its dot product with all Key tokens, followed by a softmax function to produce weights, which are then applied to the Value vectors.
Connection to Integration: The Attention Mechanism is the fundamental "engine" that allows the agent to maintain context over long reasoning chains (Section 2.6) and complex interactions with tools (Section 2.4). It enables the agent to effectively recall relevant parts of the conversation history, tool outputs, and retrieved information, making coherent decisions about subsequent actions and query analysis (Section 2.4). This capability is crucial for preventing "Context Rot" (Section 2.6) and ensuring the agent's effectiveness in dynamic environments.
Despite these structural variations, the primary training objective remains next-token prediction: given a sequence of input tokens, the model learns to predict the most probable subsequent token. This simple task, when scaled across massive datasets and parameter counts, yields emergent capabilities in reasoning, coding, and general problem-solving [25, 26].
Reasoning and Tool Use Capabilities
Beyond basic text generation, advanced LLMs exhibit significant capabilities relevant to agentic systems [27]:
* Reasoning: LLMs can engage in various forms of reasoning, from simple fact retrieval to more complex logical deduction. Techniques like Chain-of-Thought (CoT) prompting guide LLMs to break down complex problems into intermediate steps, making their reasoning process more transparent and accurate [28].
* Tool Use: A critical emergent capability is the ability of LLMs to interact with external tools or APIs. By learning to generate structured outputs (e.g., JSON function calls) in response to prompts, LLMs can extend their functionalities beyond their training data, enabling them to perform calculations, query databases, or access real-time information.
2.3 Knowledge Graphs in Software Engineering
Knowledge Graphs (KGs) provide a structured way to represent complex, interconnected data, making them particularly suitable for modeling the dependencies in software systems [29, 12]. Unlike vector stores, which capture semantic similarity, KGs capture explicit structural relationships.
Graph Data Models: RDF vs. LPG
Two primary data models exist for implementing Knowledge Graphs:
* Resource Description Framework (RDF): A W3C standard based on triples (Subject, Predicate, Object). RDF is ideal for semantic web applications and interoperability but can be verbose for traversing complex property-rich graphs typical in software engineering.
* Labeled Property Graphs (LPG): Used by graph databases like Neo4j, LPGs allow nodes and relationships to have internal properties (key-value pairs). This model is often preferred in industry for software analytics because it allows for efficient storage of metadata (e.g., a "calls" relationship can have properties like frequency or latency). This thesis utilizes the LPG model to support the rich metadata requirements of test artifacts [16].
LPG Mechanics: Index-Free Adjacency
Central to the performance of Labeled Property Graphs, particularly for traversal-heavy workloads, is the concept of Index-Free Adjacency. In this model, each node directly stores pointers to its adjacent nodes and relationships. This contrasts with index-intensive models (like RDF triple stores) where relationships are discovered through lookup tables or global indices. Index-free adjacency means that the cost of traversing a relationship is constant, $O(1)$, regardless of the total size of the graph.
Connection to Integration: This architectural choice makes the graph_traverse() tool (Section 2.4) computationally feasible for the agent's multi-hop reasoning. When the agent needs to analyze the "blast radius" of a change by exploring dependencies (e.g., from a modified function to affected tests), index-free adjacency ensures that even deep traversals complete within acceptable timeframes, preventing bottlenecks in the dynamic retrieval process.
Ontologies in Software Engineering
An ontology defines the schema or the "mental model" of the domain. In software engineering KGs, the ontology dictates the types of entities (e.g., Class, Method, TestCase, Requirement) and the allowed relationships between them (e.g., inherits_from, verifies, traces_to). A well-defined ontology is crucial for traversing the "semantic gap," allowing an agent to reason that if a Requirement is changed, the TestCases that verify it are candidates for regression testing [7, 13].
Graph Retrieval and Traversal
Retrieving information from a Knowledge Graph differs fundamentally from text retrieval. Instead of calculating similarity scores, it involves traversing the graph's structure to discover connections.
* Pattern Matching (Declarative Querying): The most common method involves expressing a desired subgraph structure using a query language like Cypher [30]. In test impact analysis, this allows for the precise selection of verification artifacts based on structural dependencies, such as retrieving "all TestCases that cover Functions called by the modified Class."
* Multi-Hop Traversal: Complex dependencies often require traversing multiple edges (hops) to find relevant information (e.g., Requirement -> Function -> Test). Algorithms like Breadth-First Search (BFS) or Depth-First Search (DFS) are used to explore the neighborhood of a starting node up to a specified depth.
* Graph Algorithms: Beyond simple traversal, advanced algorithms can be applied for global analysis. Community Detection (e.g., Leiden algorithm [31]) clusters densely connected nodes to identify functional software modules. For the agent, detecting these communities helps in understanding the broader "blast radius" of a change, suggesting that if one component in a tightly coupled cluster is modified, the entire module's test suite may require execution.
2.4 Retrieval-Augmented Generation
Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of LLMs by enabling them to fetch and incorporate relevant information from external knowledge sources during the generation process [9]. This mitigates issues like hallucinations (generative errors where the model invents facts) and improves precision by grounding the LLM's responses in factual, up-to-date data, although it introduces the risk of retrieval errors (fetching irrelevant context). In the context of test scope analysis, RAG serves as the bridge between the static knowledge contained in software artifacts (code, requirements, tests) and the dynamic reasoning capabilities of the LLM.
Embeddings and Semantic Search
The core mechanism enabling RAG is the concept of embeddings. Embeddings are numerical representations (vectors) of text, code, or other data, where semantically similar items are mapped closer to each other in a high-dimensional vector space [32]. This allows for efficient comparison and retrieval of semantically related information, surpassing keyword-based search by capturing intent and context.
Code vs. Natural Language Embeddings
While generic language models excel at natural language understanding, they often fail to capture the semantic nuance of source code or specialized software engineering terminology. This is known as the "Semantic Gap" [8]. Failure modes include:
* Tokenization Issues: Code identifiers like calculateTotalPrice are often split into calculate, total, price by natural language tokenizers, losing the original meaning.
* Structural vs. Sequential Semantics: Natural language is largely sequential, while code has rich structural relationships (e.g., class hierarchies, function calls) that are not captured by linear text embeddings.
* Domain-Specific Vocabulary: Codebases use highly specialized terms (e.g., mutex, endianness, 0xdeadbeef) that are rare or have different meanings in general text corpora.
Therefore, for effective test scope analysis, the choice of embedding model is critical. This thesis utilizes models like CodeBERT [33] or Qwen [34], which are pre-trained on vast datasets of source code, enabling them to understand code syntax and semantics more effectively than general-purpose LLMs.
Connection to Integration: Because semantic search, even with code-specific embeddings, has these inherent blind spots (e.g., struggling with precise structural queries like "all functions called by class X"), the agent must have the autonomy (Section 2.4) to bypass pure vector search in favor of graph traversal when a query indicates structural intent (Section 2.4). This adaptive selection ensures robust retrieval across diverse query types.
For technical domains, generic language models often fail to capture the semantic nuance of code or specialized terminology. Therefore, the choice of embedding model is critical. While seminal work like BERT [35] and Sentence-BERT [36] laid the foundation for contextual embeddings, the field has evolved towards massive general-purpose models. Currently, flagship proprietary models such as OpenAI's text-embedding-3 [37] and Google's gemini-embedding-001 [38] offer state-of-the-art performance. However, in sensitive software engineering contexts where data privacy is paramount, high-performing open-weight models are often preferred as they allow for secure, on-premise deployment. Notable examples include Qwen3-Embedding-8B [34] and BGE-M3 [39], which provide comparable retrieval quality while maintaining data sovereignty.
Text Retrieval Strategies
Effective RAG systems rely on robust mechanisms to locate relevant textual information. These strategies are generally categorized by how they represent and match data.
Dense Retrieval (Vector Search)
Dense retrieval operates on the semantic vector space created by embedding models. It calculates similarity metrics, typically Cosine Similarity or Euclidean Distance, between the query vector and document vectors.
* Mechanism: $score(q,d) = \cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \|\vec{B}\|}$
* Strengths: Captures semantic meaning, synonyms, and intent (e.g., mapping "login issue" to "authentication failure").
* Weaknesses: Struggles with precise keyword matching, rare entities, or specific technical identifiers (e.g., error codes like 0x800).
Sparse Retrieval (Keyword Search)
In the context of software repositories, sparse retrieval is indispensable for locating exact string matches. Unlike natural language, source code relies on precise identifiers. A developer searching for a specific error constant (e.g., ERR_TIMEOUT_503) or a unique function signature requires an exact lexical match, which dense retrievers often fail to capture due to tokenization or semantic abstraction. The seminal approach to term weighting is TF-IDF (Term Frequency-Inverse Document Frequency) [40], which assigns higher importance to terms that appear frequently in a document but rarely across the entire collection. Building upon this, the industry standard algorithm is BM25 (Best Matching 25), a probabilistic relevance framework that improves upon TF-IDF [41, 42]. Its scoring function for a document $d$ given a query $Q$ (composed of terms $q_1, \dots, q_n$) is:
$$BM25Score(d, Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, d) \cdot (k_1 + 1)}{f(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{len(d)}{avgdl})}$$
where:
* $f(q_i, d)$ is the term frequency of query term $q_i$ in document $d$.
* $len(d)$ is the length of document $d$ in words.
* $avgdl$ is the average document length in the collection.
* $k_1$ is a positive tuning parameter that calibrates term frequency saturation (typically $1.2 \le k_1 \le 2.0$). A higher $k_1$ means term frequency continues to increase relevance more strongly.
* $b$ is a parameter ($0 \le b \le 1$) that controls document length normalization. If $b = 0$, no length normalization is applied; if $b = 1$, full normalization is applied, penalizing longer documents more heavily. For code, where file lengths vary significantly, appropriate $b$ selection is crucial.
Connection to Integration: BM25 provides the lexical precision tool that the agent exposes as keyword_search() (Section 2.4). This tool is critical when the agent detects specific, precise identifiers (e.g., error codes, exact function names) in the user's query (Section 2.4), ensuring that structurally relevant but semantically distant artifacts are retrieved effectively.
Hybrid Search
For effective test scope analysis, the retrieval system must bridge the gap between high-level intent and low-level implementation. Hybrid search [43, 44] addresses this by combining the strengths of semantic search (mapping "user login" to authentication modules) with keyword search (identifying specific variables involved in a change). By fusing these results, the system ensures that relevant tests are surfaced whether they share semantic concepts or explicit code references with the modified artifacts.
A challenge in combining these disparate results is Score Fusion, where combining raw scores from algorithms with different scales (e.g., Cosine similarity is 0-1, BM25 is unbounded) is non-trivial. Therefore, this thesis adopts Reciprocal Rank Fusion (RRF) [45] as the fusion mechanism. RRF is a robust, parameter-free method that sorts documents based on their rank rather than raw score:
$$RRF_{score}(d) = \sum_{r \in R} \frac{1}{k + r(d)}$$
where $r(d)$ is the rank of document $d$ in result set $r$, and $k$ is a smoothing constant (typically 60). This ensures that documents appearing consistently high in both lists are prioritized, effectively bypassing the challenges of score normalization.
The RAG Pipeline Components
An effective RAG system for software engineering comprises several specialized components working in concert to ingest, process, and store data:
Document Loaders (Data Integration)
Document loaders are responsible for ingesting data from diverse software development lifecycle (SDLC) sources [46]. In a corporate environment, this requires "connectors" capable of interfacing with:
* Version Control Systems: Ingesting raw source code and commit history (e.g., Git).
* Documentation Platforms: Parsing structured specifications (e.g., Markdown, PDF, Internal Wikis).
* Issue Trackers: Extracting bug reports, user stories, and acceptance criteria (e.g., Jira, Linear).
These loaders normalize disparate data formats into a unified document structure suitable for processing.
Text Splitters (Semantic Chunking)
Standard text splitting (e.g., every 500 characters) is detrimental in software contexts where maintaining logical integrity is paramount, especially given the "lost-in-the-middle" phenomenon where LLMs struggle to recall information from the center of long contexts [47]. Instead, structure-aware splitting strategies are employed:
* Code-Aware Splitting: Parsing the Abstract Syntax Tree (AST) of source code to split based on functional boundaries (classes, methods, functions) rather than arbitrary line counts.
* Header-Based Splitting: Processing requirements documents by hierarchy (e.g., Section 1.2, 1.2.1), ensuring that the parent context (headers) is preserved with each child chunk.
Knowledge Stores
To effectively map the test scope, the storage layer must capture both the semantics (meaning) and the structure (dependencies) of the data.
* Vector Indices: Store the high-dimensional embeddings, enabling efficient similarity search (e.g., "Find tests related to authentication failures") [48].
* Knowledge Graphs (KGs): Explicitly model the relationships between entities (e.g., Requirement-A --verifies--> Test-B --covers--> Function-C) [29].
Vector Search (HNSW)
Efficient retrieval from large vector indices relies on Approximate Nearest Neighbor (ANN) algorithms, which sacrifice a small amount of accuracy for significant gains in search speed. This thesis employs Hierarchical Navigable Small Worlds (HNSW) graphs [49] for vector search. It is important to note that the "graph" in HNSW refers to a specialized indexing structure used purely for accelerating vector similarity search. This is distinct from the semantic Knowledge Graphs (Section 2.3), which explicitly model relationships between entities. HNSW constructs a multi-layer graph where each layer is a navigable small-world graph. Queries start at the top layer (sparsest graph), quickly navigating to a local optimum, then descending to denser layers to refine the search. This hierarchical approach offers a logarithmic time complexity ($O(\log N)$) for search operations, balancing query speed with recall performance.
Connection to Integration: HNSW is crucial for the vector_search() tool (Section 2.4) because it enables sub-second retrieval times over potentially massive code and documentation embeddings. This low-latency performance is a hard constraint for the agent's interactive Thought-Action-Observation loop, ensuring that semantic search steps do not introduce unacceptable delays in the test scope analysis process.
Integration Architecture: Dynamic Agentic Orchestration
This thesis introduces a novel agent-orchestrated architecture that dynamically integrates RAG and Knowledge Graph components for test scope analysis. Unlike static pipelines, which follow a predetermined sequence of retrieval steps, our approach leverages an AI agent to adaptively select and combine retrieval strategies based on the nuances of the user's query and the evolving information scent.
Static vs. Dynamic Retrieval Architectures
In a typical Static RAG pipeline, a user query undergoes a fixed sequence of operations—e.g., embedding, vector search, graph traversal, then summarization. While predictable and robust for homogeneous query types, this rigidity is ill-suited for the heterogeneous information landscape of software engineering. Code entities (functions, classes), issue descriptions (natural language, error codes), and trace links (structural dependencies) each demand different retrieval mechanisms.
Conversely, Dynamic Agentic Orchestration empowers an LLM-based agent to serve as a reasoning engine that autonomously decides the most effective retrieval path. This approach is grounded in Information Foraging Theory [50], which posits that agents optimize their search strategies by following "information scent"—cues in their environment that suggest the proximity and value of desired information. In our context, the agent follows the "scent" of the query to adaptively choose between semantic, structural, or hybrid retrieval modes.
Empirical motivation for this dynamic approach stems from the diverse nature of test scope analysis queries:
* Semantic-Only Queries: A query like "Tests related to authentication failures" -> Best served by Vector Search, identifying semantically similar code snippets or documentation. A graph walk alone might miss relevant tests if they aren't explicitly linked to "Auth" nodes.
* Structural-Only Queries: For "All tests calling updateUser()", Graph Traversal is optimal, leveraging explicit call graph dependencies. Vector search might struggle with lexical variations or miss tests that call wrappers of this function.
* Hybrid Queries: Complex requests such as "Tests for user login with database timeouts" necessitate a Hybrid Search, combining semantic understanding of "login" with structural analysis of "database" interactions.
A static pipeline would either always use both strategies (inefficient, adding noise) or choose one (failing on queries requiring the other), thereby underperforming across the full spectrum of user intents. Dynamic selection is thus a practical necessity for achieving comprehensive and precise test scope analysis.
Query Analysis for Strategy Selection
The agent performs intent recognition and strategy selection primarily through Chain-of-Thought (CoT) reasoning within its ReAct "Thought" step. This process leverages internal reasoning and prompt patterns to classify query types:
* Structural Signal: Queries containing phrases like "all tests calling X" or "tests affected by class Y" signal a need for graph-based structural reasoning, leading the agent to invoke graph_traverse().
* Semantic Signal: Phrases such as "tests related to Z" or "find issues about W" indicate a semantic search, prompting the agent to use vector_search().
* Hybrid Signal: Queries combining descriptive language with specific entities, e.g., "tests for X with Y", suggest a need for hybrid_search().
This emergent classification behavior, guided by the agent's system prompt and tool descriptions (detailed in Section 2.6), allows for flexible adaptation without an explicit, separate classification model.
Tool-Mediated Integration
The agent interacts with the underlying retrieval components through a set of well-defined tools, abstracting away their complexity. These tools serve as the operational interface for the agent's dynamic orchestration:
* vector_search(query: str, k: int) -> List[Document]: Queries the vector index for semantically similar documents.
* graph_traverse(start_node: str, relation: str, depth: int) -> List[Node]: Explores the Knowledge Graph based on specified starting nodes, relationship types, and traversal depth.
* hybrid_search(query: str) -> List[Result]: Combines vector and keyword search, fusing results using techniques like RRF.
These tool signatures hide the underlying algorithmic complexities (e.g., HNSW layers, BM25 tuning) from the agent, allowing it to reason at a higher level of abstraction about which tool to use rather than how to use it. This modular design also enables independent refinement of individual retrieval components without altering the agent's reasoning logic.
2.5 GraphRAG
While traditional RAG systems rely primarily on vector similarity to retrieve disjoint chunks of text, they often struggle with queries that require global reasoning or traversing complex dependencies. GraphRAG (Graph-based Retrieval-Augmented Generation) addresses this limitation by integrating Knowledge Graphs into the retrieval process.
Traditional RAG vs. GraphRAG
Traditional RAG, or Baseline RAG, follows a "retrieve-then-read" paradigm where the system fetches top-k semantically similar documents and feeds them to the LLM [9]. This approach is effective for explicit fact retrieval but often fails when the answer requires synthesizing information across multiple, indirectly connected documents (e.g., "How does a change in the billing module affect the reporting service?").
GraphRAG, specifically the approach formalized by Edge et al. [16], augments this by using the structural connections within a Knowledge Graph. It allows the system to:
* Traverse Relationships: Moving from a retrieved entity to its neighbors to gather relevant context (e.g., finding all tests linked to a modified function).
* Synthesize Global Context: Using community detection or path traversal to generate answers that reflect the broader system architecture rather than just isolated snippets.
In the context of test scope analysis, GraphRAG enables the retrieval system to "reason" about the software's structure, identifying test cases that are not textually similar to a code change but are structurally dependent on it.
2.6 AI Agents
AI Agents represent a paradigm shift from simple LLM interactions to autonomous, goal-oriented systems. An AI Agent can be conceptualized as an LLM equipped with Memory, Tools, Reasoning, and Planning capabilities. This architecture enables agents to break down complex tasks, interact with their environment, and learn from feedback.
Key Components of an AI Agent
* LLM Integration: The Large Language Model serves as the agent's brain, interpreting inputs, generating thoughts and plans, and deciding on actions. It translates high-level goals into executable steps and understands the outputs from tools.
* Memory Systems: Agents require both short-term and long-term memory to maintain context and accumulate knowledge.
   * Internal State/Short-Term Memory: This includes the current conversational context, scratchpad for intermediate thoughts, and temporary variables. Strategies like Compaction are used here to prevent context rot [51].
   * External/Long-Term Memory: Rather than a static database, long-term memory in advanced agents acts as a "curated playbook" [52]. The agent actively synthesizes lessons from past interactions, storing refined strategies and domain facts in the RAG system (Knowledge Graph), enabling it to improve over time without retraining.
* Tools: Tools are external functions, APIs, or scripts that an agent can invoke to interact with its environment, perform specific operations (e.g., search a database, execute code, call a RAG pipeline), or gather information.
* Guardrails: These are mechanisms implemented to ensure agents operate safely, reliably, and within defined ethical and operational boundaries. In the context of this thesis and deployment within a corporate environment like Ericsson, guardrails are critical for protecting proprietary intellectual property and ensuring operational integrity. They can be implemented using two complementary approaches:
   * Deterministic Guardrails: Use rule-based logic to enforce strict compliance [53]. For test scope analysis, this includes verifying that suggested test files actually exist in the repository before recommendation, or enforcing the redaction of Personally Identifiable Information (PII), such as masking sensitive user IDs in bug reports, before they are processed by the model.
   * Model-based Guardrails: Utilize LLMs to evaluate the semantic quality of inputs and outputs. Techniques such as Constitutional AI [54] use AI feedback to align models with safety principles, while specialized models like Llama Guard [55] classify content risks. In this system, model-based guardrails validate the reasoning behind test scope recommendations, ensuring the agent does not hallucinate connections between unrelated code modules.
Additionally, Human-in-the-loop (HITL) mechanisms provide a critical layer of oversight for high-stakes actions, consistent with the interactive machine learning principles outlined by Amershi et al. [56]. This architectural pattern involves pausing the agent's execution flow (interrupts) when a sensitive action is proposed. The system's state is persisted, allowing a human expert to review the request and exercise supervisory control by either approving the action, editing the parameters (e.g., refining a generated test case), or rejecting the proposal entirely with feedback.
Reasoning and Planning
Agents possess mechanisms to plan a sequence of actions to achieve a goal. The foundational paradigm is the ReAct (Reason+Act) framework [17], which enables LLMs to interleave reasoning traces ("Thoughts") with task-specific "Actions" and "Observations."
ReAct Loop and Prompt Engineering for Tool Use
The core of the ReAct paradigm is its iterative Thought-Action-Observation loop. The agent, driven by the LLM, first generates a 'Thought' based on the current goal and available information. This 'Thought' guides the subsequent 'Action', which is typically a call to an external tool. The 'Observation' is the result of that tool's execution, which then feeds back into the loop to inform the next 'Thought'.
Effective tool use in this loop is heavily reliant on precise Prompt Engineering. The LLM is provided with a System Prompt that acts as its operational manual, defining its persona, overall objective, and crucially, the interface to its available tools. This system prompt typically includes:
* Agent Persona and Goal: Setting the context for the agent's role (e.g., "You are an expert test scope analysis agent...") and its primary objective.
* Available Tools and Descriptions: A structured list of callable functions (e.g., vector_search, graph_traverse, hybrid_search), along with clear, concise descriptions of their purpose, input parameters, and expected output types. These tool definitions are critical; they enable the agent to understand when and how to invoke each tool.
* Output Format Constraints: Guiding the LLM to generate tool calls in a specific, parseable format (e.g., JSON).
This detailed prompt structure enables the emergent classification behavior described in Section 2.4. By analyzing the user's query and the current state, the agent's 'Thought' process identifies the most appropriate tool or sequence of tools, operationalizing the dynamic retrieval strategy. The quality of these tool definitions directly impacts the agent's ability to accurately map user intent to effective retrieval actions.
The ACE Cycle
Building on this, advanced systems employ the ACE cycle [52] for self-improvement: Generation (of the plan), Reflection (critiquing the plan/result), and Curation (saving the lesson to memory).
Context Engineering
Effective RAG and Agentic systems are not merely about retrieving data, but about adapting it to maximize the LLM's reasoning capabilities. Context Engineering represents a systematic framework for this adaptation, treating context not as a static buffer but as an evolving playbook for the agent. It is distinct from Prompt Engineering, which focuses on optimizing the instructions given to the model. Context Engineering, conversely, focuses on the architecture and state management of the information (context window) supplied to the model to support complex reasoning tasks [51, 52].
The Challenge of Context Rot
As systems scale, simply retrieving more data leads to "Context Rot," a phenomenon where the model's ability to recall and reason about specific information degrades as the context window fills with distractors [51]. This aligns with the concept of "Brevity Bias," where critical domain insights are lost amidst noise [52]. Therefore, treating context as a finite, high-value resource is essential.
The ACE Framework
To address these limitations, we adopt principles from the Agentic Context Engineering (ACE) framework [52]. ACE treats context construction as a modular process involving:
* Generation: Retrieving and drafting initial context blocks based on the query.
* Reflection: Analyzing the retrieved data to identify gaps or inconsistencies.
* Curation: Iteratively refining and structuring the context to create a coherent narrative or "playbook" for the agent.
Optimization Strategies
Practical implementation relies on specific strategies to combat rot and maintain long-horizon coherence [51]:
* Compaction: Periodically summarizing conversation history or verbose tool outputs to retain only the essential state changes, freeing up tokens for new reasoning.
* Structured Note-Taking: Empowering the agent to explicitly "write down" key facts or decisions into a dedicated memory block (or Knowledge Graph), rather than relying on implicit recall from a long transcript.
* Delimited Evidence: Using clear XML-style tags (e.g., <code_snippet>) to demarcate external data from internal logic, preventing the model from confusing retrieved evidence with system instructions.
2.7 Evaluation Metrics
To rigorously assess the performance of the proposed retrieval system, standard metrics from the field of Information Retrieval (IR) are employed. The primary goal in test scope analysis is to present relevant tests to the engineer (high recall) while minimizing the noise of irrelevant results (high precision) within the limited window of user attention [57].
Rank-Aware Metrics
Since the system provides a recommended list of tests, the order of results is critical. A relevant test appearing at position 50 is effectively "missed" by a busy engineer. Therefore, we focus on rank-aware metrics:
* Recall@k: Measures the proportion of relevant items retrieved within the top $k$ results (e.g., $k=10$). This is the primary safety metric, indicating the system's ability to surface correct tests within the user's immediate view.
* Precision@k: Measures the proportion of items in the top $k$ results that are actually relevant. High precision@k ensures that the user does not waste time reviewing irrelevant suggestions.
* Mean Average Precision (MAP): While Recall@k focuses on a specific cutoff, MAP provides a single-figure measure of quality across recall levels. It calculates the average precision at the position of every relevant item, rewarding systems that place relevant tests higher in the list. This metric is widely regarded as the standard for evaluating ranked retrieval systems [57].